<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>scrolls.ast.tokenizer API documentation</title>
<meta name="description" content="The tokenizer implementation …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>scrolls.ast.tokenizer</code></h1>
</header>
<section id="section-intro">
<p>The tokenizer implementation.</p>
<p>See <a href="../ast/index.html#tokenizing">ast: Tokenizing</a>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
The tokenizer implementation.

See [ast: Tokenizing](../ast/index.html#tokenizing).
&#34;&#34;&#34;

import dataclasses
import logging
import types
import typing as t

from scrolls import errors as base_errors

from . import ast_errors, streams
from .ast_constants import (BLOCK_CLOSE, BLOCK_OPEN, CLOSE_ARGS, COMMAND_SEP,
                            COMMENT_SIGIL, CONTROL_SIGIL, EOF, ESCAPE_SIGIL,
                            EXPANSION_SIGIL, OPEN_ARGS, QUOTE, SPREAD_SIGIL,
                            TokenizeConsumeRestState, TokenType)

__all__ = (
    &#34;Token&#34;,
    &#34;Tokenizer&#34;
)


logger = logging.getLogger(__name__)


def _str_ensure(s: str, ensure: str) -&gt; str:
    if ensure not in s:
        return s + ensure
    else:
        return s


def _str_remove(s: str, remove: str) -&gt; str:
    return s.replace(remove, &#34;&#34;)


def _str_switch(s: str, switch: str, en: bool) -&gt; str:
    &#34;&#34;&#34;
    Utility function for enabling/disabling detection of certain characters.
    &#34;&#34;&#34;

    if en:
        return _str_ensure(s, switch)
    else:
        return _str_remove(s, switch)


@dataclasses.dataclass
class Token:
    &#34;&#34;&#34;A token.&#34;&#34;&#34;

    type: TokenType
    &#34;&#34;&#34;The type of this token.&#34;&#34;&#34;

    value: str
    &#34;&#34;&#34;The value of this token.&#34;&#34;&#34;

    line: int
    &#34;&#34;&#34;The line this token *started* generating on. Some tokens may span multiple lines.&#34;&#34;&#34;

    position: int
    &#34;&#34;&#34;The column along the line that this token *started* generating on. &#34;&#34;&#34;

    tokenizer: &#34;Tokenizer&#34;
    &#34;&#34;&#34;The tokenizer that generated this token.&#34;&#34;&#34;

    consume_rest: bool = False
    &#34;&#34;&#34;Sets whether this token was generated by CONSUME_REST.&#34;&#34;&#34;

    def __str__(self) -&gt; str:
        return f&#34;{self.type.name}:{repr(self.value)}&#34;


class Tokenizer:
    &#34;&#34;&#34;
    The tokenizer. This class is responsible for identifying meaningful pieces of scripts
    (such as string literals, block open and close, etc.), and tagging them.

    .. WARNING::
        If the tokenizer is supplied with a string, then this `Tokenizer` is **single use**.
        If you wish to stream input, implement a `scrolls.ast.streams.CharStream`. See
        `scrolls.ast.streams.StringStream.feed` and see if that works for you.
        See `scrolls.ast.streams.REPLStream` for an example of streaming input
        from a user.

    Args:
        stream: The script to tokenize. This may be a string or a `scrolls.ast.streams.CharStream` instance.
        consume_rest_triggers: Triggers for CONSUME_REST.
    &#34;&#34;&#34;
    def __init__(
        self,
        stream: t.Union[str, streams.CharStream],
        consume_rest_triggers: t.Mapping[str, int] = types.MappingProxyType({})
    ):
        if isinstance(stream, str):
            self.stream: streams.CharStream = streams.StringStream(stream.strip())
        else:
            self.stream = stream

        self.consume_rest_triggers = consume_rest_triggers
        self.consume_rest_state = TokenizeConsumeRestState.OFF
        self.consume_rest_count = 0
        self.previous_token_was_sep = True
        self.whitespace = &#34;\t &#34;

        # Map of single characters to token types
        self.charmap = {
            &#34;\n&#34;: TokenType.COMMAND_SEP,
            &#34;;&#34;: TokenType.COMMAND_SEP,
            OPEN_ARGS: TokenType.OPEN_ARGS,
            CLOSE_ARGS: TokenType.CLOSE_ARGS,
            BLOCK_OPEN: TokenType.OPEN_BLOCK,
            BLOCK_CLOSE: TokenType.CLOSE_BLOCK,
            EXPANSION_SIGIL: TokenType.EXPANSION_SIGIL,
            CONTROL_SIGIL: TokenType.CONTROL_SIGIL,
            SPREAD_SIGIL: TokenType.SPREAD_SIGIL
        }

        self.escape_sequences: t.MutableMapping[
            str,
            t.Union[str, t.Callable[[Tokenizer], str]]
        ] = {
            &#34;n&#34;: &#34;\n&#34;,
            &#34;t&#34;: &#34;\t&#34;,
            &#34;r&#34;: &#34;\r&#34;,
            ESCAPE_SIGIL: ESCAPE_SIGIL,
            QUOTE: QUOTE,
            &#34;u&#34;: Tokenizer._unicode_escape
        }

        # Set up stop chars for unquoted string literals.
        self._string_literal_always_stop = self.whitespace
        self._string_literal_stop_quoted = QUOTE
        self._string_literal_stop_comment = COMMENT_SIGIL

        # Note: Add an exception for newlines. Even when we don&#39;t consider newlines to be command separators,
        # we would normally want newlines to separate string literals. So remove \n from this switch.
        self._string_literal_stop_single_char = _str_remove(
            &#34;&#34;.join(self.charmap.keys()),
            &#34;\n&#34;
        )

        # Override flag for behavior when single_char_token_enable is False.
        self.newlines_separate_strings = True

        self.string_literal_stop: str = self._string_literal_always_stop
        self.single_char_token_enable = True
        self.set_single_char_token_enable(True)

        # Set up stop chars for CONSUME_REST.
        self._consume_rest_stop_switch: str = &#34;&#34;.join([*COMMAND_SEP, BLOCK_CLOSE, BLOCK_OPEN])
        self.consume_rest_stop: str = &#34;&#34;
        self.set_consume_rest_all(False)

        # Set up stop chars for quoted literals.
        self.quoted_literal_stop: str = QUOTE  # For now, quoted literals ONLY stop on another quote.
        self.quoted_literal_enable = True
        self.set_quoted_literals_enable(True)

        # Set up stop chars for comments. (Note: No need for specific comment stop char here, it&#39;s hardcoded to
        # be \n at the moment.)
        self.comments_enable = True
        self.set_comments_enable(True)

    def _unicode_escape(self) -&gt; str:
        code_point = &#34;&#34;  # Initialization not needed, just satisfies some linters.
        try:
            code_point = self.next_n_chars(4)
        except ast_errors.TokenizeEofError:
            self.error(
                ast_errors.TokenizeEofError,
                &#34;Ran off end of script trying to parse unicode escape.&#34;
            )

        if QUOTE in code_point:
            self.error(
                ast_errors.TokenizeError,
                f&#34;Encountered {QUOTE} while consuming unicode escape.&#34;,
                pos=self.stream.current_pos() - 4
            )

        char = &#34;&#34;
        try:
            char = chr(int(code_point, 16))
        except ValueError:
            self.error(
                ast_errors.TokenizeError,
                f&#34;Bad hex number {code_point}.&#34;,
                pos=self.stream.current_pos() - 4
            )

        return char

    def set_consume_rest_all(self, consume_all: bool) -&gt; None:
        &#34;&#34;&#34;
        Set whether CONSUME_REST consumes until EOF. Defaults to `False`.

        If `False`, CONSUME_REST will stop on block open/close, and command separators.

        If `True`, CONSUME_REST will not stop until EOF.
        &#34;&#34;&#34;
        self.consume_rest_stop = _str_switch(
            self.consume_rest_stop,
            self._consume_rest_stop_switch,
            not consume_all
        )

    def set_single_char_token_enable(self, en: bool) -&gt; None:
        &#34;&#34;&#34;
        Set whether single character tokens should be parsed. This includes ALL token types except for
        `scrolls.ast.ast_constants.TokenType.STRING_LITERAL` and `scrolls.ast.ast_constants.TokenType.COMMENT`.
        Defaults to `True`.

        If `False`, then all special characters that would otherwise be their own token will be rolled
        into string literals.
        &#34;&#34;&#34;
        if self.newlines_separate_strings and not self.single_char_token_enable and en:
            # If we&#39;re re-enabling single char tokens and the newline separator behavior is still on,
            # we need to undo that first.
            self.set_newlines_separate_strings(False)

        self.single_char_token_enable = en

        self.string_literal_stop = _str_switch(
            self.string_literal_stop,
            self._string_literal_stop_single_char,
            en
        )

        if not en:
            self.set_newlines_separate_strings(True)
        else:
            # If single char tokens are enabled, newlines must stop string literals for this to work properly.
            self.string_literal_stop = _str_ensure(self.string_literal_stop, &#34;\n&#34;)

    def set_quoted_literals_enable(self, en: bool) -&gt; None:
        &#34;&#34;&#34;
        Set whether quoted string literals are enabled. If disabled, quotes will be rolled into normal string token
        parsing.

        For instance, if quoted literals are disabled, `&#34;Hello World&#34;` would be interpreted as `&#34;Hello`, `World&#34;`.
        &#34;&#34;&#34;
        self.quoted_literal_enable = en
        self.string_literal_stop = _str_switch(
            self.string_literal_stop,
            self._string_literal_stop_quoted,
            en
        )

    def set_comments_enable(self, en: bool) -&gt; None:
        &#34;&#34;&#34;
        Set whether comments are enabled. If disabled, the comment character will be ignored, and anything that
        would be a comment will be treated as ordinary code.
        &#34;&#34;&#34;
        self.comments_enable = en
        self.string_literal_stop = _str_switch(
            self.string_literal_stop,
            self._string_literal_stop_comment,
            en
        )

    def set_newlines_separate_strings(self, en: bool) -&gt; None:
        &#34;&#34;&#34;
        Set whether newlines separate string literals. This can only be modified if
        `Tokenizer.set_single_char_token_enable` has been set to `False`, and will raise a
        `scrolls.errors.ScrollError` otherwise.

        By default, when `Tokenizer.set_single_char_token_enable` is set to `False`, newlines will instead be
        considered whitespace, and will separate strings without producing
        `scrolls.ast.ast_constants.TokenType.COMMAND_SEP` tokens.

        To override this behavior, this function may be set to `False`. In this case, newlines will be rolled into
        string literals, and ONLY spaces and tabs will separate string literals.
        &#34;&#34;&#34;
        if self.single_char_token_enable:
            raise base_errors.ScrollError(&#34;Cannot use set_newlines_separate_strings when single char tokens are enabled.&#34;)

        self.newlines_separate_strings = en
        self.string_literal_stop = _str_switch(
            self.string_literal_stop,
            &#34;\n&#34;,
            en
        )
        self.whitespace = _str_switch(
            self.whitespace,
            &#34;\n&#34;,
            en
        )

    def error(
        self,
        err_type: t.Type[base_errors.PositionalError],
        message: str,
        line: t.Optional[int] = None,
        pos: t.Optional[int] = None
    ) -&gt; t.NoReturn:
        if line is not None:
            _line = line
        else:
            _line = self.stream.current_line()

        if pos is not None:
            _pos = pos
        else:
            _pos = self.stream.current_pos()

        raise err_type(
            _line,
            _pos,
            self.stream.history(),
            message
        )

    def forbid_eof(self, msg: str = &#34;&#34;, *args: t.Any, **kwargs: t.Any) -&gt; None:
        if not msg:
            msg = &#34;Unexpected EOF while parsing script.&#34;

        if self.stream.at_eof() or self.stream.after_eof():
            self.error(ast_errors.TokenizeEofError, msg.format(*args, **kwargs))

    def next_n_chars(self, n: int) -&gt; str:
        &#34;&#34;&#34;
        Unconditionally consume N characters and return them.
        &#34;&#34;&#34;
        chars: t.MutableSequence[str] = []
        for _ in range(n):
            self.forbid_eof(
                &#34;Ran into EOF while consuming characters. Got {}, wanted {}.&#34;,
                len(chars), n
            )

            chars.append(self.stream.get_char())
            self.stream.next_char()

        return &#34;&#34;.join(chars)

    # Get a single char token.
    def accept_single_char(self) -&gt; t.Optional[Token]:
        if not self.single_char_token_enable:
            return None

        char = self.stream.get_char()

        if char in self.charmap:
            tok = Token(
                self.charmap[char],
                char,
                self.stream.current_line(),
                self.stream.current_pos(),
                self
            )
            self.stream.next_char()
            return tok

        return None

    def accept_eof(self) -&gt; t.Optional[Token]:
        if self.stream.at_eof():
            # Once an EOF is generated, there are no more tokens.
            # Any attempts after this to generate a token will
            # result in an exception.
            self.stream.next_char()  # Put stream into after eof state

            return Token(
                TokenType.EOF,
                EOF,
                self.stream.current_line(),
                self.stream.current_pos(),
                self
            )
        else:
            return None

    def accept_whitespace(self) -&gt; t.Optional[Token]:
        char = self.stream.get_char()
        if char in self.whitespace:
            self.stream.next_char()
            return Token(
                TokenType.WHITESPACE,
                char,
                self.stream.current_line(),
                self.stream.current_pos(),
                self
            )

        return None

    def try_consume_escape(self) -&gt; t.Optional[str]:
        if self.stream.get_char() != ESCAPE_SIGIL:
            return None

        self.stream.next_char()
        self.forbid_eof()

        escape_char = self.stream.get_char()
        if escape_char not in self.escape_sequences:
            self.error(ast_errors.TokenizeError, f&#34;Invalid escape &#39;{escape_char}&#39;&#34;)

        self.stream.next_char()
        self.forbid_eof()

        replacement = self.escape_sequences[escape_char]
        if isinstance(replacement, str):
            return replacement
        elif callable(replacement):
            return replacement(self)
        else:
            raise TypeError(f&#34;Bad type for escape sequence {escape_char}, &#34;
                            &#34;must be &#39;str&#39; or &#39;(Tokenizer) -&gt; str&#39;&#34;)

    def accept_string_literal(
        self,
        stop_chars: t.Sequence[str] = (),
        error_on_eof: bool = False,
        allow_escapes: bool = False
    ) -&gt; t.Optional[Token]:
        self.forbid_eof(&#34;String literal should not start on EOF&#34;)

        char = self.stream.get_char()
        pos = self.stream.current_pos()
        line = self.stream.current_line()
        chars = []

        while char not in stop_chars:
            if allow_escapes:
                escape = self.try_consume_escape()
                if escape is not None:
                    chars.append(escape)
                    char = self.stream.get_char()
                    continue

            chars.append(char)
            self.stream.next_char()
            if self.stream.at_eof():
                if error_on_eof:
                    self.error(
                        ast_errors.TokenizeEofError,
                        &#34;Unexpected EOF while parsing string literal.&#34;
                    )
                else:
                    break

            char = self.stream.get_char()

        return Token(
            TokenType.STRING_LITERAL,
            &#34;&#34;.join(chars),
            line,
            pos,
            self
        )

    def accept_comment(self) -&gt; t.Optional[Token]:
        if not self.comments_enable:
            return None

        char = self.stream.get_char()
        pos = self.stream.current_pos()
        line = self.stream.current_line()
        chars = []

        if char != COMMENT_SIGIL:
            return None

        self.stream.next_char()
        while char != &#34;\n&#34;:
            chars.append(char)
            self.stream.next_char()

            if self.stream.at_eof():
                break

            char = self.stream.get_char()

        return Token(
            TokenType.COMMENT,
            &#34;&#34;.join(chars),
            line,
            pos,
            self
        )

    # Accepts a normal string literal. No CONSUME_REST, not quoted.
    def accept_string_literal_normal(self) -&gt; t.Optional[Token]:
        return self.accept_string_literal(
            stop_chars=self.string_literal_stop,
            error_on_eof=False  # Just stop on EOF, no errors.
        )

    # Accept a CONSUME_REST literal.
    def accept_string_literal_consume_rest(self) -&gt; t.Optional[Token]:
        return self.accept_string_literal(
            stop_chars=self.consume_rest_stop,
            error_on_eof=False  # Stop on EOF. No errors.
        )

    # Accept a quoted string literal.
    def accept_string_literal_quoted(self) -&gt; t.Optional[Token]:
        if not self.quoted_literal_enable:
            return None

        if self.stream.get_char() != QUOTE:
            return None
        else:
            self.stream.next_char()

        literal = self.accept_string_literal(
            stop_chars=self.quoted_literal_stop,
            error_on_eof=True,  # Quoted literals must be closed.
            allow_escapes=True  # Escapes only allowed in quoted literals.
        )

        if literal is None:
            self.error(
                ast_errors.TokenizeError,
                &#34;internal: Got None from accept_string_literal, shouldn&#39;t have.&#34;
            )

        if self.stream.get_char() != QUOTE:
            self.error(
                ast_errors.TokenizeError,
                &#34;internal: Missing end quote, should have resulted in EOF error.&#34;
            )
        else:
            self.stream.next_char()

        return literal

    @staticmethod
    def accept_any_of(*f: t.Callable[[], t.Optional[Token]]) -&gt; t.Optional[Token]:
        for fun in f:
            tok = fun()
            if tok is not None:
                return tok

        return None

    def handle_consume_rest_off(self, tok: Token) -&gt; None:
        if tok.type in (TokenType.COMMAND_SEP, TokenType.CLOSE_BLOCK, TokenType.CLOSE_ARGS):
            self.previous_token_was_sep = True
            return

        # Test to see if we should enter CONSUME_REST state.
        # Only trigger CONSUME_REST if the previous token was a command separator.
        should_enter_consume_rest = (
                self.previous_token_was_sep and
                tok.type == TokenType.STRING_LITERAL and
                tok.value in self.consume_rest_triggers
        )
        self.previous_token_was_sep = False
        if should_enter_consume_rest:
            count = self.consume_rest_triggers[tok.value]

            if count == 0:
                self.consume_rest_state = TokenizeConsumeRestState.CONSUME
            else:
                self.consume_rest_state = TokenizeConsumeRestState.COUNTING
                self.consume_rest_count = count

    def handle_consume_rest_counting(self, tok: Token) -&gt; None:
        self.previous_token_was_sep = False

        # Only count down on string literals.
        if tok.type == TokenType.STRING_LITERAL:
            self.consume_rest_count -= 1

            # Once countdown is over, CONSUME_REST on next token.
            if self.consume_rest_count == 0:
                self.consume_rest_state = TokenizeConsumeRestState.CONSUME

        # If we get any other token type, then cancel CONSUME_REST
        else:
            self.consume_rest_state = TokenizeConsumeRestState.OFF
            self.consume_rest_count = 0

    def handle_consume_rest_consume(self, tok: Token) -&gt; None:
        # This function runs AFTER a CONSUME_REST consumption. So, just set consume_rest back to OFF.
        self.consume_rest_state = TokenizeConsumeRestState.OFF
        self.consume_rest_count = 0

    # TODO
    # Consume rest state handler. All this code is pretty ugly, and does not account
    # for more advanced usage.
    def handle_consume_rest(self, tok: Token) -&gt; None:
        f_map: t.Mapping[TokenizeConsumeRestState, t.Callable[[Token], None]] = {
            TokenizeConsumeRestState.OFF: self.handle_consume_rest_off,
            TokenizeConsumeRestState.COUNTING: self.handle_consume_rest_counting,
            TokenizeConsumeRestState.CONSUME: self.handle_consume_rest_consume
        }

        f_map[self.consume_rest_state](tok)

    def next_token(self) -&gt; Token:
        &#34;&#34;&#34;
        Extract the next token. If the tokenizing is finished, this will return a `Token` of type
        `scrolls.ast.ast_constants.TokenType.EOF`

        Raises:
            `scrolls.ast.ast_errors.TokenizeEofError`: If EOF was hit unexpectedly.
            `scrolls.ast.ast_errors.TokenizeError`: If a generic issue happened while tokenizing.
        &#34;&#34;&#34;
        if self.consume_rest_state == TokenizeConsumeRestState.CONSUME:
            while True:
                tok = self.accept_any_of(
                    self.accept_whitespace
                )

                if tok is None:
                    break

                if tok.type == TokenType.WHITESPACE:
                    continue

            tok = self.accept_string_literal_consume_rest()
            if tok is None:
                self.error(
                    ast_errors.TokenizeError,
                    &#34;Got bad string literal during consume_rest&#34;
                )
            logger.debug(f&#34;tokenize: Got token {tok.type.name}:{repr(tok.value)}&#34;)
            tok.consume_rest = True  # Signal we got this token using CONSUME_REST

            self.handle_consume_rest(tok)
            return tok
        else:
            while True:
                if self.stream.after_eof():
                    self.error(
                        ast_errors.TokenizeEofError,
                        &#34;No more tokens.&#34;
                    )

                tok = None

                try:
                    tok = self.accept_any_of(
                        self.accept_whitespace,
                        self.accept_comment,
                        self.accept_single_char,
                        self.accept_string_literal_quoted,
                        self.accept_string_literal_normal
                    )
                except ast_errors.StreamEofError:
                    # !!! HACK
                    # I really, really need to rethink how EOF is handled
                    # throughout this entire module. It&#39;s broken.
                    pass

                if tok is None:
                    # If tok is None, then all tokenizing functions got
                    # rejected. So, try to accept and return EOF.

                    eof_tok = self.accept_eof()

                    if eof_tok is None:
                        self.error(
                            ast_errors.TokenizeError,
                            &#34;Unexpectedly rejected all tokenizing functions.&#34;
                        )
                    else:
                        return eof_tok

                # Loop until we get a non-whitespace, non-comment token.
                if tok.type not in [TokenType.WHITESPACE, TokenType.COMMENT]:
                    logger.debug(f&#34;tokenize: Got token {tok.type.name}:{repr(tok.value)}&#34;)
                    self.handle_consume_rest(tok)
                    return tok

    def get_all_tokens(self) -&gt; t.Sequence[Token]:
        &#34;&#34;&#34;
        Extracts all tokens at once, until the end of the script. A sequence of tokens obtained this way
        will always end with a token of type `scrolls.ast.ast_constants.TokenType.EOF`.

        Raises:
            `scrolls.ast.ast_errors.TokenizeEofError`: If EOF was hit unexpectedly.
            `scrolls.ast.ast_errors.TokenizeError`: If a generic issue happened while tokenizing.
        &#34;&#34;&#34;
        tokens: t.MutableSequence[Token] = []

        while True:
            tok = self.next_token()
            tokens.append(tok)
            if tok.type == TokenType.EOF:
                return tokens</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="scrolls.ast.tokenizer.Token"><code class="flex name class">
<span>class <span class="ident">Token</span></span>
<span>(</span><span>type: <a title="scrolls.ast.ast_constants.TokenType" href="ast_constants.html#scrolls.ast.ast_constants.TokenType">TokenType</a>, value: str, line: int, position: int, tokenizer: <a title="scrolls.ast.tokenizer.Tokenizer" href="#scrolls.ast.tokenizer.Tokenizer">Tokenizer</a>, consume_rest: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>A token.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclasses.dataclass
class Token:
    &#34;&#34;&#34;A token.&#34;&#34;&#34;

    type: TokenType
    &#34;&#34;&#34;The type of this token.&#34;&#34;&#34;

    value: str
    &#34;&#34;&#34;The value of this token.&#34;&#34;&#34;

    line: int
    &#34;&#34;&#34;The line this token *started* generating on. Some tokens may span multiple lines.&#34;&#34;&#34;

    position: int
    &#34;&#34;&#34;The column along the line that this token *started* generating on. &#34;&#34;&#34;

    tokenizer: &#34;Tokenizer&#34;
    &#34;&#34;&#34;The tokenizer that generated this token.&#34;&#34;&#34;

    consume_rest: bool = False
    &#34;&#34;&#34;Sets whether this token was generated by CONSUME_REST.&#34;&#34;&#34;

    def __str__(self) -&gt; str:
        return f&#34;{self.type.name}:{repr(self.value)}&#34;</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="scrolls.ast.tokenizer.Token.consume_rest"><code class="name">var <span class="ident">consume_rest</span> : bool</code></dt>
<dd>
<div class="desc"><p>Sets whether this token was generated by CONSUME_REST.</p></div>
</dd>
<dt id="scrolls.ast.tokenizer.Token.line"><code class="name">var <span class="ident">line</span> : int</code></dt>
<dd>
<div class="desc"><p>The line this token <em>started</em> generating on. Some tokens may span multiple lines.</p></div>
</dd>
<dt id="scrolls.ast.tokenizer.Token.position"><code class="name">var <span class="ident">position</span> : int</code></dt>
<dd>
<div class="desc"><p>The column along the line that this token <em>started</em> generating on.</p></div>
</dd>
<dt id="scrolls.ast.tokenizer.Token.tokenizer"><code class="name">var <span class="ident">tokenizer</span> : <a title="scrolls.ast.tokenizer.Tokenizer" href="#scrolls.ast.tokenizer.Tokenizer">Tokenizer</a></code></dt>
<dd>
<div class="desc"><p>The tokenizer that generated this token.</p></div>
</dd>
<dt id="scrolls.ast.tokenizer.Token.type"><code class="name">var <span class="ident">type</span> : <a title="scrolls.ast.ast_constants.TokenType" href="ast_constants.html#scrolls.ast.ast_constants.TokenType">TokenType</a></code></dt>
<dd>
<div class="desc"><p>The type of this token.</p></div>
</dd>
<dt id="scrolls.ast.tokenizer.Token.value"><code class="name">var <span class="ident">value</span> : str</code></dt>
<dd>
<div class="desc"><p>The value of this token.</p></div>
</dd>
</dl>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer"><code class="flex name class">
<span>class <span class="ident">Tokenizer</span></span>
<span>(</span><span>stream: Union[str, <a title="scrolls.ast.streams.CharStream" href="streams.html#scrolls.ast.streams.CharStream">CharStream</a>], consume_rest_triggers: Mapping[str, int] = mappingproxy({}))</span>
</code></dt>
<dd>
<div class="desc"><p>The tokenizer. This class is responsible for identifying meaningful pieces of scripts
(such as string literals, block open and close, etc.), and tagging them.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If the tokenizer is supplied with a string, then this <code><a title="scrolls.ast.tokenizer.Tokenizer" href="#scrolls.ast.tokenizer.Tokenizer">Tokenizer</a></code> is <strong>single use</strong>.
If you wish to stream input, implement a <code><a title="scrolls.ast.streams.CharStream" href="streams.html#scrolls.ast.streams.CharStream">CharStream</a></code>. See
<code><a title="scrolls.ast.streams.StringStream.feed" href="streams.html#scrolls.ast.streams.StringStream.feed">StringStream.feed()</a></code> and see if that works for you.
See <code><a title="scrolls.ast.streams.REPLStream" href="streams.html#scrolls.ast.streams.REPLStream">REPLStream</a></code> for an example of streaming input
from a user.</p>
</div>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>stream</code></strong></dt>
<dd>The script to tokenize. This may be a string or a <code><a title="scrolls.ast.streams.CharStream" href="streams.html#scrolls.ast.streams.CharStream">CharStream</a></code> instance.</dd>
<dt><strong><code>consume_rest_triggers</code></strong></dt>
<dd>Triggers for CONSUME_REST.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tokenizer:
    &#34;&#34;&#34;
    The tokenizer. This class is responsible for identifying meaningful pieces of scripts
    (such as string literals, block open and close, etc.), and tagging them.

    .. WARNING::
        If the tokenizer is supplied with a string, then this `Tokenizer` is **single use**.
        If you wish to stream input, implement a `scrolls.ast.streams.CharStream`. See
        `scrolls.ast.streams.StringStream.feed` and see if that works for you.
        See `scrolls.ast.streams.REPLStream` for an example of streaming input
        from a user.

    Args:
        stream: The script to tokenize. This may be a string or a `scrolls.ast.streams.CharStream` instance.
        consume_rest_triggers: Triggers for CONSUME_REST.
    &#34;&#34;&#34;
    def __init__(
        self,
        stream: t.Union[str, streams.CharStream],
        consume_rest_triggers: t.Mapping[str, int] = types.MappingProxyType({})
    ):
        if isinstance(stream, str):
            self.stream: streams.CharStream = streams.StringStream(stream.strip())
        else:
            self.stream = stream

        self.consume_rest_triggers = consume_rest_triggers
        self.consume_rest_state = TokenizeConsumeRestState.OFF
        self.consume_rest_count = 0
        self.previous_token_was_sep = True
        self.whitespace = &#34;\t &#34;

        # Map of single characters to token types
        self.charmap = {
            &#34;\n&#34;: TokenType.COMMAND_SEP,
            &#34;;&#34;: TokenType.COMMAND_SEP,
            OPEN_ARGS: TokenType.OPEN_ARGS,
            CLOSE_ARGS: TokenType.CLOSE_ARGS,
            BLOCK_OPEN: TokenType.OPEN_BLOCK,
            BLOCK_CLOSE: TokenType.CLOSE_BLOCK,
            EXPANSION_SIGIL: TokenType.EXPANSION_SIGIL,
            CONTROL_SIGIL: TokenType.CONTROL_SIGIL,
            SPREAD_SIGIL: TokenType.SPREAD_SIGIL
        }

        self.escape_sequences: t.MutableMapping[
            str,
            t.Union[str, t.Callable[[Tokenizer], str]]
        ] = {
            &#34;n&#34;: &#34;\n&#34;,
            &#34;t&#34;: &#34;\t&#34;,
            &#34;r&#34;: &#34;\r&#34;,
            ESCAPE_SIGIL: ESCAPE_SIGIL,
            QUOTE: QUOTE,
            &#34;u&#34;: Tokenizer._unicode_escape
        }

        # Set up stop chars for unquoted string literals.
        self._string_literal_always_stop = self.whitespace
        self._string_literal_stop_quoted = QUOTE
        self._string_literal_stop_comment = COMMENT_SIGIL

        # Note: Add an exception for newlines. Even when we don&#39;t consider newlines to be command separators,
        # we would normally want newlines to separate string literals. So remove \n from this switch.
        self._string_literal_stop_single_char = _str_remove(
            &#34;&#34;.join(self.charmap.keys()),
            &#34;\n&#34;
        )

        # Override flag for behavior when single_char_token_enable is False.
        self.newlines_separate_strings = True

        self.string_literal_stop: str = self._string_literal_always_stop
        self.single_char_token_enable = True
        self.set_single_char_token_enable(True)

        # Set up stop chars for CONSUME_REST.
        self._consume_rest_stop_switch: str = &#34;&#34;.join([*COMMAND_SEP, BLOCK_CLOSE, BLOCK_OPEN])
        self.consume_rest_stop: str = &#34;&#34;
        self.set_consume_rest_all(False)

        # Set up stop chars for quoted literals.
        self.quoted_literal_stop: str = QUOTE  # For now, quoted literals ONLY stop on another quote.
        self.quoted_literal_enable = True
        self.set_quoted_literals_enable(True)

        # Set up stop chars for comments. (Note: No need for specific comment stop char here, it&#39;s hardcoded to
        # be \n at the moment.)
        self.comments_enable = True
        self.set_comments_enable(True)

    def _unicode_escape(self) -&gt; str:
        code_point = &#34;&#34;  # Initialization not needed, just satisfies some linters.
        try:
            code_point = self.next_n_chars(4)
        except ast_errors.TokenizeEofError:
            self.error(
                ast_errors.TokenizeEofError,
                &#34;Ran off end of script trying to parse unicode escape.&#34;
            )

        if QUOTE in code_point:
            self.error(
                ast_errors.TokenizeError,
                f&#34;Encountered {QUOTE} while consuming unicode escape.&#34;,
                pos=self.stream.current_pos() - 4
            )

        char = &#34;&#34;
        try:
            char = chr(int(code_point, 16))
        except ValueError:
            self.error(
                ast_errors.TokenizeError,
                f&#34;Bad hex number {code_point}.&#34;,
                pos=self.stream.current_pos() - 4
            )

        return char

    def set_consume_rest_all(self, consume_all: bool) -&gt; None:
        &#34;&#34;&#34;
        Set whether CONSUME_REST consumes until EOF. Defaults to `False`.

        If `False`, CONSUME_REST will stop on block open/close, and command separators.

        If `True`, CONSUME_REST will not stop until EOF.
        &#34;&#34;&#34;
        self.consume_rest_stop = _str_switch(
            self.consume_rest_stop,
            self._consume_rest_stop_switch,
            not consume_all
        )

    def set_single_char_token_enable(self, en: bool) -&gt; None:
        &#34;&#34;&#34;
        Set whether single character tokens should be parsed. This includes ALL token types except for
        `scrolls.ast.ast_constants.TokenType.STRING_LITERAL` and `scrolls.ast.ast_constants.TokenType.COMMENT`.
        Defaults to `True`.

        If `False`, then all special characters that would otherwise be their own token will be rolled
        into string literals.
        &#34;&#34;&#34;
        if self.newlines_separate_strings and not self.single_char_token_enable and en:
            # If we&#39;re re-enabling single char tokens and the newline separator behavior is still on,
            # we need to undo that first.
            self.set_newlines_separate_strings(False)

        self.single_char_token_enable = en

        self.string_literal_stop = _str_switch(
            self.string_literal_stop,
            self._string_literal_stop_single_char,
            en
        )

        if not en:
            self.set_newlines_separate_strings(True)
        else:
            # If single char tokens are enabled, newlines must stop string literals for this to work properly.
            self.string_literal_stop = _str_ensure(self.string_literal_stop, &#34;\n&#34;)

    def set_quoted_literals_enable(self, en: bool) -&gt; None:
        &#34;&#34;&#34;
        Set whether quoted string literals are enabled. If disabled, quotes will be rolled into normal string token
        parsing.

        For instance, if quoted literals are disabled, `&#34;Hello World&#34;` would be interpreted as `&#34;Hello`, `World&#34;`.
        &#34;&#34;&#34;
        self.quoted_literal_enable = en
        self.string_literal_stop = _str_switch(
            self.string_literal_stop,
            self._string_literal_stop_quoted,
            en
        )

    def set_comments_enable(self, en: bool) -&gt; None:
        &#34;&#34;&#34;
        Set whether comments are enabled. If disabled, the comment character will be ignored, and anything that
        would be a comment will be treated as ordinary code.
        &#34;&#34;&#34;
        self.comments_enable = en
        self.string_literal_stop = _str_switch(
            self.string_literal_stop,
            self._string_literal_stop_comment,
            en
        )

    def set_newlines_separate_strings(self, en: bool) -&gt; None:
        &#34;&#34;&#34;
        Set whether newlines separate string literals. This can only be modified if
        `Tokenizer.set_single_char_token_enable` has been set to `False`, and will raise a
        `scrolls.errors.ScrollError` otherwise.

        By default, when `Tokenizer.set_single_char_token_enable` is set to `False`, newlines will instead be
        considered whitespace, and will separate strings without producing
        `scrolls.ast.ast_constants.TokenType.COMMAND_SEP` tokens.

        To override this behavior, this function may be set to `False`. In this case, newlines will be rolled into
        string literals, and ONLY spaces and tabs will separate string literals.
        &#34;&#34;&#34;
        if self.single_char_token_enable:
            raise base_errors.ScrollError(&#34;Cannot use set_newlines_separate_strings when single char tokens are enabled.&#34;)

        self.newlines_separate_strings = en
        self.string_literal_stop = _str_switch(
            self.string_literal_stop,
            &#34;\n&#34;,
            en
        )
        self.whitespace = _str_switch(
            self.whitespace,
            &#34;\n&#34;,
            en
        )

    def error(
        self,
        err_type: t.Type[base_errors.PositionalError],
        message: str,
        line: t.Optional[int] = None,
        pos: t.Optional[int] = None
    ) -&gt; t.NoReturn:
        if line is not None:
            _line = line
        else:
            _line = self.stream.current_line()

        if pos is not None:
            _pos = pos
        else:
            _pos = self.stream.current_pos()

        raise err_type(
            _line,
            _pos,
            self.stream.history(),
            message
        )

    def forbid_eof(self, msg: str = &#34;&#34;, *args: t.Any, **kwargs: t.Any) -&gt; None:
        if not msg:
            msg = &#34;Unexpected EOF while parsing script.&#34;

        if self.stream.at_eof() or self.stream.after_eof():
            self.error(ast_errors.TokenizeEofError, msg.format(*args, **kwargs))

    def next_n_chars(self, n: int) -&gt; str:
        &#34;&#34;&#34;
        Unconditionally consume N characters and return them.
        &#34;&#34;&#34;
        chars: t.MutableSequence[str] = []
        for _ in range(n):
            self.forbid_eof(
                &#34;Ran into EOF while consuming characters. Got {}, wanted {}.&#34;,
                len(chars), n
            )

            chars.append(self.stream.get_char())
            self.stream.next_char()

        return &#34;&#34;.join(chars)

    # Get a single char token.
    def accept_single_char(self) -&gt; t.Optional[Token]:
        if not self.single_char_token_enable:
            return None

        char = self.stream.get_char()

        if char in self.charmap:
            tok = Token(
                self.charmap[char],
                char,
                self.stream.current_line(),
                self.stream.current_pos(),
                self
            )
            self.stream.next_char()
            return tok

        return None

    def accept_eof(self) -&gt; t.Optional[Token]:
        if self.stream.at_eof():
            # Once an EOF is generated, there are no more tokens.
            # Any attempts after this to generate a token will
            # result in an exception.
            self.stream.next_char()  # Put stream into after eof state

            return Token(
                TokenType.EOF,
                EOF,
                self.stream.current_line(),
                self.stream.current_pos(),
                self
            )
        else:
            return None

    def accept_whitespace(self) -&gt; t.Optional[Token]:
        char = self.stream.get_char()
        if char in self.whitespace:
            self.stream.next_char()
            return Token(
                TokenType.WHITESPACE,
                char,
                self.stream.current_line(),
                self.stream.current_pos(),
                self
            )

        return None

    def try_consume_escape(self) -&gt; t.Optional[str]:
        if self.stream.get_char() != ESCAPE_SIGIL:
            return None

        self.stream.next_char()
        self.forbid_eof()

        escape_char = self.stream.get_char()
        if escape_char not in self.escape_sequences:
            self.error(ast_errors.TokenizeError, f&#34;Invalid escape &#39;{escape_char}&#39;&#34;)

        self.stream.next_char()
        self.forbid_eof()

        replacement = self.escape_sequences[escape_char]
        if isinstance(replacement, str):
            return replacement
        elif callable(replacement):
            return replacement(self)
        else:
            raise TypeError(f&#34;Bad type for escape sequence {escape_char}, &#34;
                            &#34;must be &#39;str&#39; or &#39;(Tokenizer) -&gt; str&#39;&#34;)

    def accept_string_literal(
        self,
        stop_chars: t.Sequence[str] = (),
        error_on_eof: bool = False,
        allow_escapes: bool = False
    ) -&gt; t.Optional[Token]:
        self.forbid_eof(&#34;String literal should not start on EOF&#34;)

        char = self.stream.get_char()
        pos = self.stream.current_pos()
        line = self.stream.current_line()
        chars = []

        while char not in stop_chars:
            if allow_escapes:
                escape = self.try_consume_escape()
                if escape is not None:
                    chars.append(escape)
                    char = self.stream.get_char()
                    continue

            chars.append(char)
            self.stream.next_char()
            if self.stream.at_eof():
                if error_on_eof:
                    self.error(
                        ast_errors.TokenizeEofError,
                        &#34;Unexpected EOF while parsing string literal.&#34;
                    )
                else:
                    break

            char = self.stream.get_char()

        return Token(
            TokenType.STRING_LITERAL,
            &#34;&#34;.join(chars),
            line,
            pos,
            self
        )

    def accept_comment(self) -&gt; t.Optional[Token]:
        if not self.comments_enable:
            return None

        char = self.stream.get_char()
        pos = self.stream.current_pos()
        line = self.stream.current_line()
        chars = []

        if char != COMMENT_SIGIL:
            return None

        self.stream.next_char()
        while char != &#34;\n&#34;:
            chars.append(char)
            self.stream.next_char()

            if self.stream.at_eof():
                break

            char = self.stream.get_char()

        return Token(
            TokenType.COMMENT,
            &#34;&#34;.join(chars),
            line,
            pos,
            self
        )

    # Accepts a normal string literal. No CONSUME_REST, not quoted.
    def accept_string_literal_normal(self) -&gt; t.Optional[Token]:
        return self.accept_string_literal(
            stop_chars=self.string_literal_stop,
            error_on_eof=False  # Just stop on EOF, no errors.
        )

    # Accept a CONSUME_REST literal.
    def accept_string_literal_consume_rest(self) -&gt; t.Optional[Token]:
        return self.accept_string_literal(
            stop_chars=self.consume_rest_stop,
            error_on_eof=False  # Stop on EOF. No errors.
        )

    # Accept a quoted string literal.
    def accept_string_literal_quoted(self) -&gt; t.Optional[Token]:
        if not self.quoted_literal_enable:
            return None

        if self.stream.get_char() != QUOTE:
            return None
        else:
            self.stream.next_char()

        literal = self.accept_string_literal(
            stop_chars=self.quoted_literal_stop,
            error_on_eof=True,  # Quoted literals must be closed.
            allow_escapes=True  # Escapes only allowed in quoted literals.
        )

        if literal is None:
            self.error(
                ast_errors.TokenizeError,
                &#34;internal: Got None from accept_string_literal, shouldn&#39;t have.&#34;
            )

        if self.stream.get_char() != QUOTE:
            self.error(
                ast_errors.TokenizeError,
                &#34;internal: Missing end quote, should have resulted in EOF error.&#34;
            )
        else:
            self.stream.next_char()

        return literal

    @staticmethod
    def accept_any_of(*f: t.Callable[[], t.Optional[Token]]) -&gt; t.Optional[Token]:
        for fun in f:
            tok = fun()
            if tok is not None:
                return tok

        return None

    def handle_consume_rest_off(self, tok: Token) -&gt; None:
        if tok.type in (TokenType.COMMAND_SEP, TokenType.CLOSE_BLOCK, TokenType.CLOSE_ARGS):
            self.previous_token_was_sep = True
            return

        # Test to see if we should enter CONSUME_REST state.
        # Only trigger CONSUME_REST if the previous token was a command separator.
        should_enter_consume_rest = (
                self.previous_token_was_sep and
                tok.type == TokenType.STRING_LITERAL and
                tok.value in self.consume_rest_triggers
        )
        self.previous_token_was_sep = False
        if should_enter_consume_rest:
            count = self.consume_rest_triggers[tok.value]

            if count == 0:
                self.consume_rest_state = TokenizeConsumeRestState.CONSUME
            else:
                self.consume_rest_state = TokenizeConsumeRestState.COUNTING
                self.consume_rest_count = count

    def handle_consume_rest_counting(self, tok: Token) -&gt; None:
        self.previous_token_was_sep = False

        # Only count down on string literals.
        if tok.type == TokenType.STRING_LITERAL:
            self.consume_rest_count -= 1

            # Once countdown is over, CONSUME_REST on next token.
            if self.consume_rest_count == 0:
                self.consume_rest_state = TokenizeConsumeRestState.CONSUME

        # If we get any other token type, then cancel CONSUME_REST
        else:
            self.consume_rest_state = TokenizeConsumeRestState.OFF
            self.consume_rest_count = 0

    def handle_consume_rest_consume(self, tok: Token) -&gt; None:
        # This function runs AFTER a CONSUME_REST consumption. So, just set consume_rest back to OFF.
        self.consume_rest_state = TokenizeConsumeRestState.OFF
        self.consume_rest_count = 0

    # TODO
    # Consume rest state handler. All this code is pretty ugly, and does not account
    # for more advanced usage.
    def handle_consume_rest(self, tok: Token) -&gt; None:
        f_map: t.Mapping[TokenizeConsumeRestState, t.Callable[[Token], None]] = {
            TokenizeConsumeRestState.OFF: self.handle_consume_rest_off,
            TokenizeConsumeRestState.COUNTING: self.handle_consume_rest_counting,
            TokenizeConsumeRestState.CONSUME: self.handle_consume_rest_consume
        }

        f_map[self.consume_rest_state](tok)

    def next_token(self) -&gt; Token:
        &#34;&#34;&#34;
        Extract the next token. If the tokenizing is finished, this will return a `Token` of type
        `scrolls.ast.ast_constants.TokenType.EOF`

        Raises:
            `scrolls.ast.ast_errors.TokenizeEofError`: If EOF was hit unexpectedly.
            `scrolls.ast.ast_errors.TokenizeError`: If a generic issue happened while tokenizing.
        &#34;&#34;&#34;
        if self.consume_rest_state == TokenizeConsumeRestState.CONSUME:
            while True:
                tok = self.accept_any_of(
                    self.accept_whitespace
                )

                if tok is None:
                    break

                if tok.type == TokenType.WHITESPACE:
                    continue

            tok = self.accept_string_literal_consume_rest()
            if tok is None:
                self.error(
                    ast_errors.TokenizeError,
                    &#34;Got bad string literal during consume_rest&#34;
                )
            logger.debug(f&#34;tokenize: Got token {tok.type.name}:{repr(tok.value)}&#34;)
            tok.consume_rest = True  # Signal we got this token using CONSUME_REST

            self.handle_consume_rest(tok)
            return tok
        else:
            while True:
                if self.stream.after_eof():
                    self.error(
                        ast_errors.TokenizeEofError,
                        &#34;No more tokens.&#34;
                    )

                tok = None

                try:
                    tok = self.accept_any_of(
                        self.accept_whitespace,
                        self.accept_comment,
                        self.accept_single_char,
                        self.accept_string_literal_quoted,
                        self.accept_string_literal_normal
                    )
                except ast_errors.StreamEofError:
                    # !!! HACK
                    # I really, really need to rethink how EOF is handled
                    # throughout this entire module. It&#39;s broken.
                    pass

                if tok is None:
                    # If tok is None, then all tokenizing functions got
                    # rejected. So, try to accept and return EOF.

                    eof_tok = self.accept_eof()

                    if eof_tok is None:
                        self.error(
                            ast_errors.TokenizeError,
                            &#34;Unexpectedly rejected all tokenizing functions.&#34;
                        )
                    else:
                        return eof_tok

                # Loop until we get a non-whitespace, non-comment token.
                if tok.type not in [TokenType.WHITESPACE, TokenType.COMMENT]:
                    logger.debug(f&#34;tokenize: Got token {tok.type.name}:{repr(tok.value)}&#34;)
                    self.handle_consume_rest(tok)
                    return tok

    def get_all_tokens(self) -&gt; t.Sequence[Token]:
        &#34;&#34;&#34;
        Extracts all tokens at once, until the end of the script. A sequence of tokens obtained this way
        will always end with a token of type `scrolls.ast.ast_constants.TokenType.EOF`.

        Raises:
            `scrolls.ast.ast_errors.TokenizeEofError`: If EOF was hit unexpectedly.
            `scrolls.ast.ast_errors.TokenizeError`: If a generic issue happened while tokenizing.
        &#34;&#34;&#34;
        tokens: t.MutableSequence[Token] = []

        while True:
            tok = self.next_token()
            tokens.append(tok)
            if tok.type == TokenType.EOF:
                return tokens</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="scrolls.ast.tokenizer.Tokenizer.accept_any_of"><code class="name flex">
<span>def <span class="ident">accept_any_of</span></span>(<span>*f: Callable[[], Optional[<a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>]]) ‑> Optional[<a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def accept_any_of(*f: t.Callable[[], t.Optional[Token]]) -&gt; t.Optional[Token]:
    for fun in f:
        tok = fun()
        if tok is not None:
            return tok

    return None</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scrolls.ast.tokenizer.Tokenizer.accept_comment"><code class="name flex">
<span>def <span class="ident">accept_comment</span></span>(<span>self) ‑> Optional[<a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accept_comment(self) -&gt; t.Optional[Token]:
    if not self.comments_enable:
        return None

    char = self.stream.get_char()
    pos = self.stream.current_pos()
    line = self.stream.current_line()
    chars = []

    if char != COMMENT_SIGIL:
        return None

    self.stream.next_char()
    while char != &#34;\n&#34;:
        chars.append(char)
        self.stream.next_char()

        if self.stream.at_eof():
            break

        char = self.stream.get_char()

    return Token(
        TokenType.COMMENT,
        &#34;&#34;.join(chars),
        line,
        pos,
        self
    )</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.accept_eof"><code class="name flex">
<span>def <span class="ident">accept_eof</span></span>(<span>self) ‑> Optional[<a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accept_eof(self) -&gt; t.Optional[Token]:
    if self.stream.at_eof():
        # Once an EOF is generated, there are no more tokens.
        # Any attempts after this to generate a token will
        # result in an exception.
        self.stream.next_char()  # Put stream into after eof state

        return Token(
            TokenType.EOF,
            EOF,
            self.stream.current_line(),
            self.stream.current_pos(),
            self
        )
    else:
        return None</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.accept_single_char"><code class="name flex">
<span>def <span class="ident">accept_single_char</span></span>(<span>self) ‑> Optional[<a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accept_single_char(self) -&gt; t.Optional[Token]:
    if not self.single_char_token_enable:
        return None

    char = self.stream.get_char()

    if char in self.charmap:
        tok = Token(
            self.charmap[char],
            char,
            self.stream.current_line(),
            self.stream.current_pos(),
            self
        )
        self.stream.next_char()
        return tok

    return None</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.accept_string_literal"><code class="name flex">
<span>def <span class="ident">accept_string_literal</span></span>(<span>self, stop_chars: Sequence[str] = (), error_on_eof: bool = False, allow_escapes: bool = False) ‑> Optional[<a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accept_string_literal(
    self,
    stop_chars: t.Sequence[str] = (),
    error_on_eof: bool = False,
    allow_escapes: bool = False
) -&gt; t.Optional[Token]:
    self.forbid_eof(&#34;String literal should not start on EOF&#34;)

    char = self.stream.get_char()
    pos = self.stream.current_pos()
    line = self.stream.current_line()
    chars = []

    while char not in stop_chars:
        if allow_escapes:
            escape = self.try_consume_escape()
            if escape is not None:
                chars.append(escape)
                char = self.stream.get_char()
                continue

        chars.append(char)
        self.stream.next_char()
        if self.stream.at_eof():
            if error_on_eof:
                self.error(
                    ast_errors.TokenizeEofError,
                    &#34;Unexpected EOF while parsing string literal.&#34;
                )
            else:
                break

        char = self.stream.get_char()

    return Token(
        TokenType.STRING_LITERAL,
        &#34;&#34;.join(chars),
        line,
        pos,
        self
    )</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.accept_string_literal_consume_rest"><code class="name flex">
<span>def <span class="ident">accept_string_literal_consume_rest</span></span>(<span>self) ‑> Optional[<a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accept_string_literal_consume_rest(self) -&gt; t.Optional[Token]:
    return self.accept_string_literal(
        stop_chars=self.consume_rest_stop,
        error_on_eof=False  # Stop on EOF. No errors.
    )</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.accept_string_literal_normal"><code class="name flex">
<span>def <span class="ident">accept_string_literal_normal</span></span>(<span>self) ‑> Optional[<a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accept_string_literal_normal(self) -&gt; t.Optional[Token]:
    return self.accept_string_literal(
        stop_chars=self.string_literal_stop,
        error_on_eof=False  # Just stop on EOF, no errors.
    )</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.accept_string_literal_quoted"><code class="name flex">
<span>def <span class="ident">accept_string_literal_quoted</span></span>(<span>self) ‑> Optional[<a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accept_string_literal_quoted(self) -&gt; t.Optional[Token]:
    if not self.quoted_literal_enable:
        return None

    if self.stream.get_char() != QUOTE:
        return None
    else:
        self.stream.next_char()

    literal = self.accept_string_literal(
        stop_chars=self.quoted_literal_stop,
        error_on_eof=True,  # Quoted literals must be closed.
        allow_escapes=True  # Escapes only allowed in quoted literals.
    )

    if literal is None:
        self.error(
            ast_errors.TokenizeError,
            &#34;internal: Got None from accept_string_literal, shouldn&#39;t have.&#34;
        )

    if self.stream.get_char() != QUOTE:
        self.error(
            ast_errors.TokenizeError,
            &#34;internal: Missing end quote, should have resulted in EOF error.&#34;
        )
    else:
        self.stream.next_char()

    return literal</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.accept_whitespace"><code class="name flex">
<span>def <span class="ident">accept_whitespace</span></span>(<span>self) ‑> Optional[<a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accept_whitespace(self) -&gt; t.Optional[Token]:
    char = self.stream.get_char()
    if char in self.whitespace:
        self.stream.next_char()
        return Token(
            TokenType.WHITESPACE,
            char,
            self.stream.current_line(),
            self.stream.current_pos(),
            self
        )

    return None</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.error"><code class="name flex">
<span>def <span class="ident">error</span></span>(<span>self, err_type: Type[<a title="scrolls.errors.PositionalError" href="../errors.html#scrolls.errors.PositionalError">PositionalError</a>], message: str, line: Optional[int] = None, pos: Optional[int] = None) ‑> NoReturn</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def error(
    self,
    err_type: t.Type[base_errors.PositionalError],
    message: str,
    line: t.Optional[int] = None,
    pos: t.Optional[int] = None
) -&gt; t.NoReturn:
    if line is not None:
        _line = line
    else:
        _line = self.stream.current_line()

    if pos is not None:
        _pos = pos
    else:
        _pos = self.stream.current_pos()

    raise err_type(
        _line,
        _pos,
        self.stream.history(),
        message
    )</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.forbid_eof"><code class="name flex">
<span>def <span class="ident">forbid_eof</span></span>(<span>self, msg: str = '', *args: Any, **kwargs: Any) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forbid_eof(self, msg: str = &#34;&#34;, *args: t.Any, **kwargs: t.Any) -&gt; None:
    if not msg:
        msg = &#34;Unexpected EOF while parsing script.&#34;

    if self.stream.at_eof() or self.stream.after_eof():
        self.error(ast_errors.TokenizeEofError, msg.format(*args, **kwargs))</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.get_all_tokens"><code class="name flex">
<span>def <span class="ident">get_all_tokens</span></span>(<span>self) ‑> Sequence[<a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts all tokens at once, until the end of the script. A sequence of tokens obtained this way
will always end with a token of type <code><a title="scrolls.ast.ast_constants.TokenType.EOF" href="ast_constants.html#scrolls.ast.ast_constants.TokenType.EOF">TokenType.EOF</a></code>.</p>
<h2 id="raises">Raises</h2>
<p><code><a title="scrolls.ast.ast_errors.TokenizeEofError" href="ast_errors.html#scrolls.ast.ast_errors.TokenizeEofError">TokenizeEofError</a></code>: If EOF was hit unexpectedly.
<code><a title="scrolls.ast.ast_errors.TokenizeError" href="ast_errors.html#scrolls.ast.ast_errors.TokenizeError">TokenizeError</a></code>: If a generic issue happened while tokenizing.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_all_tokens(self) -&gt; t.Sequence[Token]:
    &#34;&#34;&#34;
    Extracts all tokens at once, until the end of the script. A sequence of tokens obtained this way
    will always end with a token of type `scrolls.ast.ast_constants.TokenType.EOF`.

    Raises:
        `scrolls.ast.ast_errors.TokenizeEofError`: If EOF was hit unexpectedly.
        `scrolls.ast.ast_errors.TokenizeError`: If a generic issue happened while tokenizing.
    &#34;&#34;&#34;
    tokens: t.MutableSequence[Token] = []

    while True:
        tok = self.next_token()
        tokens.append(tok)
        if tok.type == TokenType.EOF:
            return tokens</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.handle_consume_rest"><code class="name flex">
<span>def <span class="ident">handle_consume_rest</span></span>(<span>self, tok: <a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle_consume_rest(self, tok: Token) -&gt; None:
    f_map: t.Mapping[TokenizeConsumeRestState, t.Callable[[Token], None]] = {
        TokenizeConsumeRestState.OFF: self.handle_consume_rest_off,
        TokenizeConsumeRestState.COUNTING: self.handle_consume_rest_counting,
        TokenizeConsumeRestState.CONSUME: self.handle_consume_rest_consume
    }

    f_map[self.consume_rest_state](tok)</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.handle_consume_rest_consume"><code class="name flex">
<span>def <span class="ident">handle_consume_rest_consume</span></span>(<span>self, tok: <a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle_consume_rest_consume(self, tok: Token) -&gt; None:
    # This function runs AFTER a CONSUME_REST consumption. So, just set consume_rest back to OFF.
    self.consume_rest_state = TokenizeConsumeRestState.OFF
    self.consume_rest_count = 0</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.handle_consume_rest_counting"><code class="name flex">
<span>def <span class="ident">handle_consume_rest_counting</span></span>(<span>self, tok: <a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle_consume_rest_counting(self, tok: Token) -&gt; None:
    self.previous_token_was_sep = False

    # Only count down on string literals.
    if tok.type == TokenType.STRING_LITERAL:
        self.consume_rest_count -= 1

        # Once countdown is over, CONSUME_REST on next token.
        if self.consume_rest_count == 0:
            self.consume_rest_state = TokenizeConsumeRestState.CONSUME

    # If we get any other token type, then cancel CONSUME_REST
    else:
        self.consume_rest_state = TokenizeConsumeRestState.OFF
        self.consume_rest_count = 0</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.handle_consume_rest_off"><code class="name flex">
<span>def <span class="ident">handle_consume_rest_off</span></span>(<span>self, tok: <a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a>) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle_consume_rest_off(self, tok: Token) -&gt; None:
    if tok.type in (TokenType.COMMAND_SEP, TokenType.CLOSE_BLOCK, TokenType.CLOSE_ARGS):
        self.previous_token_was_sep = True
        return

    # Test to see if we should enter CONSUME_REST state.
    # Only trigger CONSUME_REST if the previous token was a command separator.
    should_enter_consume_rest = (
            self.previous_token_was_sep and
            tok.type == TokenType.STRING_LITERAL and
            tok.value in self.consume_rest_triggers
    )
    self.previous_token_was_sep = False
    if should_enter_consume_rest:
        count = self.consume_rest_triggers[tok.value]

        if count == 0:
            self.consume_rest_state = TokenizeConsumeRestState.CONSUME
        else:
            self.consume_rest_state = TokenizeConsumeRestState.COUNTING
            self.consume_rest_count = count</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.next_n_chars"><code class="name flex">
<span>def <span class="ident">next_n_chars</span></span>(<span>self, n: int) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Unconditionally consume N characters and return them.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def next_n_chars(self, n: int) -&gt; str:
    &#34;&#34;&#34;
    Unconditionally consume N characters and return them.
    &#34;&#34;&#34;
    chars: t.MutableSequence[str] = []
    for _ in range(n):
        self.forbid_eof(
            &#34;Ran into EOF while consuming characters. Got {}, wanted {}.&#34;,
            len(chars), n
        )

        chars.append(self.stream.get_char())
        self.stream.next_char()

    return &#34;&#34;.join(chars)</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.next_token"><code class="name flex">
<span>def <span class="ident">next_token</span></span>(<span>self) ‑> <a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a></span>
</code></dt>
<dd>
<div class="desc"><p>Extract the next token. If the tokenizing is finished, this will return a <code><a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a></code> of type
<code><a title="scrolls.ast.ast_constants.TokenType.EOF" href="ast_constants.html#scrolls.ast.ast_constants.TokenType.EOF">TokenType.EOF</a></code></p>
<h2 id="raises">Raises</h2>
<p><code><a title="scrolls.ast.ast_errors.TokenizeEofError" href="ast_errors.html#scrolls.ast.ast_errors.TokenizeEofError">TokenizeEofError</a></code>: If EOF was hit unexpectedly.
<code><a title="scrolls.ast.ast_errors.TokenizeError" href="ast_errors.html#scrolls.ast.ast_errors.TokenizeError">TokenizeError</a></code>: If a generic issue happened while tokenizing.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def next_token(self) -&gt; Token:
    &#34;&#34;&#34;
    Extract the next token. If the tokenizing is finished, this will return a `Token` of type
    `scrolls.ast.ast_constants.TokenType.EOF`

    Raises:
        `scrolls.ast.ast_errors.TokenizeEofError`: If EOF was hit unexpectedly.
        `scrolls.ast.ast_errors.TokenizeError`: If a generic issue happened while tokenizing.
    &#34;&#34;&#34;
    if self.consume_rest_state == TokenizeConsumeRestState.CONSUME:
        while True:
            tok = self.accept_any_of(
                self.accept_whitespace
            )

            if tok is None:
                break

            if tok.type == TokenType.WHITESPACE:
                continue

        tok = self.accept_string_literal_consume_rest()
        if tok is None:
            self.error(
                ast_errors.TokenizeError,
                &#34;Got bad string literal during consume_rest&#34;
            )
        logger.debug(f&#34;tokenize: Got token {tok.type.name}:{repr(tok.value)}&#34;)
        tok.consume_rest = True  # Signal we got this token using CONSUME_REST

        self.handle_consume_rest(tok)
        return tok
    else:
        while True:
            if self.stream.after_eof():
                self.error(
                    ast_errors.TokenizeEofError,
                    &#34;No more tokens.&#34;
                )

            tok = None

            try:
                tok = self.accept_any_of(
                    self.accept_whitespace,
                    self.accept_comment,
                    self.accept_single_char,
                    self.accept_string_literal_quoted,
                    self.accept_string_literal_normal
                )
            except ast_errors.StreamEofError:
                # !!! HACK
                # I really, really need to rethink how EOF is handled
                # throughout this entire module. It&#39;s broken.
                pass

            if tok is None:
                # If tok is None, then all tokenizing functions got
                # rejected. So, try to accept and return EOF.

                eof_tok = self.accept_eof()

                if eof_tok is None:
                    self.error(
                        ast_errors.TokenizeError,
                        &#34;Unexpectedly rejected all tokenizing functions.&#34;
                    )
                else:
                    return eof_tok

            # Loop until we get a non-whitespace, non-comment token.
            if tok.type not in [TokenType.WHITESPACE, TokenType.COMMENT]:
                logger.debug(f&#34;tokenize: Got token {tok.type.name}:{repr(tok.value)}&#34;)
                self.handle_consume_rest(tok)
                return tok</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.set_comments_enable"><code class="name flex">
<span>def <span class="ident">set_comments_enable</span></span>(<span>self, en: bool) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Set whether comments are enabled. If disabled, the comment character will be ignored, and anything that
would be a comment will be treated as ordinary code.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_comments_enable(self, en: bool) -&gt; None:
    &#34;&#34;&#34;
    Set whether comments are enabled. If disabled, the comment character will be ignored, and anything that
    would be a comment will be treated as ordinary code.
    &#34;&#34;&#34;
    self.comments_enable = en
    self.string_literal_stop = _str_switch(
        self.string_literal_stop,
        self._string_literal_stop_comment,
        en
    )</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.set_consume_rest_all"><code class="name flex">
<span>def <span class="ident">set_consume_rest_all</span></span>(<span>self, consume_all: bool) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Set whether CONSUME_REST consumes until EOF. Defaults to <code>False</code>.</p>
<p>If <code>False</code>, CONSUME_REST will stop on block open/close, and command separators.</p>
<p>If <code>True</code>, CONSUME_REST will not stop until EOF.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_consume_rest_all(self, consume_all: bool) -&gt; None:
    &#34;&#34;&#34;
    Set whether CONSUME_REST consumes until EOF. Defaults to `False`.

    If `False`, CONSUME_REST will stop on block open/close, and command separators.

    If `True`, CONSUME_REST will not stop until EOF.
    &#34;&#34;&#34;
    self.consume_rest_stop = _str_switch(
        self.consume_rest_stop,
        self._consume_rest_stop_switch,
        not consume_all
    )</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.set_newlines_separate_strings"><code class="name flex">
<span>def <span class="ident">set_newlines_separate_strings</span></span>(<span>self, en: bool) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Set whether newlines separate string literals. This can only be modified if
<code><a title="scrolls.ast.tokenizer.Tokenizer.set_single_char_token_enable" href="#scrolls.ast.tokenizer.Tokenizer.set_single_char_token_enable">Tokenizer.set_single_char_token_enable()</a></code> has been set to <code>False</code>, and will raise a
<code><a title="scrolls.errors.ScrollError" href="../errors.html#scrolls.errors.ScrollError">ScrollError</a></code> otherwise.</p>
<p>By default, when <code><a title="scrolls.ast.tokenizer.Tokenizer.set_single_char_token_enable" href="#scrolls.ast.tokenizer.Tokenizer.set_single_char_token_enable">Tokenizer.set_single_char_token_enable()</a></code> is set to <code>False</code>, newlines will instead be
considered whitespace, and will separate strings without producing
<code><a title="scrolls.ast.ast_constants.TokenType.COMMAND_SEP" href="ast_constants.html#scrolls.ast.ast_constants.TokenType.COMMAND_SEP">TokenType.COMMAND_SEP</a></code> tokens.</p>
<p>To override this behavior, this function may be set to <code>False</code>. In this case, newlines will be rolled into
string literals, and ONLY spaces and tabs will separate string literals.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_newlines_separate_strings(self, en: bool) -&gt; None:
    &#34;&#34;&#34;
    Set whether newlines separate string literals. This can only be modified if
    `Tokenizer.set_single_char_token_enable` has been set to `False`, and will raise a
    `scrolls.errors.ScrollError` otherwise.

    By default, when `Tokenizer.set_single_char_token_enable` is set to `False`, newlines will instead be
    considered whitespace, and will separate strings without producing
    `scrolls.ast.ast_constants.TokenType.COMMAND_SEP` tokens.

    To override this behavior, this function may be set to `False`. In this case, newlines will be rolled into
    string literals, and ONLY spaces and tabs will separate string literals.
    &#34;&#34;&#34;
    if self.single_char_token_enable:
        raise base_errors.ScrollError(&#34;Cannot use set_newlines_separate_strings when single char tokens are enabled.&#34;)

    self.newlines_separate_strings = en
    self.string_literal_stop = _str_switch(
        self.string_literal_stop,
        &#34;\n&#34;,
        en
    )
    self.whitespace = _str_switch(
        self.whitespace,
        &#34;\n&#34;,
        en
    )</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.set_quoted_literals_enable"><code class="name flex">
<span>def <span class="ident">set_quoted_literals_enable</span></span>(<span>self, en: bool) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Set whether quoted string literals are enabled. If disabled, quotes will be rolled into normal string token
parsing.</p>
<p>For instance, if quoted literals are disabled, <code>"Hello World"</code> would be interpreted as <code>"Hello</code>, <code>World"</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_quoted_literals_enable(self, en: bool) -&gt; None:
    &#34;&#34;&#34;
    Set whether quoted string literals are enabled. If disabled, quotes will be rolled into normal string token
    parsing.

    For instance, if quoted literals are disabled, `&#34;Hello World&#34;` would be interpreted as `&#34;Hello`, `World&#34;`.
    &#34;&#34;&#34;
    self.quoted_literal_enable = en
    self.string_literal_stop = _str_switch(
        self.string_literal_stop,
        self._string_literal_stop_quoted,
        en
    )</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.set_single_char_token_enable"><code class="name flex">
<span>def <span class="ident">set_single_char_token_enable</span></span>(<span>self, en: bool) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Set whether single character tokens should be parsed. This includes ALL token types except for
<code><a title="scrolls.ast.ast_constants.TokenType.STRING_LITERAL" href="ast_constants.html#scrolls.ast.ast_constants.TokenType.STRING_LITERAL">TokenType.STRING_LITERAL</a></code> and <code><a title="scrolls.ast.ast_constants.TokenType.COMMENT" href="ast_constants.html#scrolls.ast.ast_constants.TokenType.COMMENT">TokenType.COMMENT</a></code>.
Defaults to <code>True</code>.</p>
<p>If <code>False</code>, then all special characters that would otherwise be their own token will be rolled
into string literals.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_single_char_token_enable(self, en: bool) -&gt; None:
    &#34;&#34;&#34;
    Set whether single character tokens should be parsed. This includes ALL token types except for
    `scrolls.ast.ast_constants.TokenType.STRING_LITERAL` and `scrolls.ast.ast_constants.TokenType.COMMENT`.
    Defaults to `True`.

    If `False`, then all special characters that would otherwise be their own token will be rolled
    into string literals.
    &#34;&#34;&#34;
    if self.newlines_separate_strings and not self.single_char_token_enable and en:
        # If we&#39;re re-enabling single char tokens and the newline separator behavior is still on,
        # we need to undo that first.
        self.set_newlines_separate_strings(False)

    self.single_char_token_enable = en

    self.string_literal_stop = _str_switch(
        self.string_literal_stop,
        self._string_literal_stop_single_char,
        en
    )

    if not en:
        self.set_newlines_separate_strings(True)
    else:
        # If single char tokens are enabled, newlines must stop string literals for this to work properly.
        self.string_literal_stop = _str_ensure(self.string_literal_stop, &#34;\n&#34;)</code></pre>
</details>
</dd>
<dt id="scrolls.ast.tokenizer.Tokenizer.try_consume_escape"><code class="name flex">
<span>def <span class="ident">try_consume_escape</span></span>(<span>self) ‑> Optional[str]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def try_consume_escape(self) -&gt; t.Optional[str]:
    if self.stream.get_char() != ESCAPE_SIGIL:
        return None

    self.stream.next_char()
    self.forbid_eof()

    escape_char = self.stream.get_char()
    if escape_char not in self.escape_sequences:
        self.error(ast_errors.TokenizeError, f&#34;Invalid escape &#39;{escape_char}&#39;&#34;)

    self.stream.next_char()
    self.forbid_eof()

    replacement = self.escape_sequences[escape_char]
    if isinstance(replacement, str):
        return replacement
    elif callable(replacement):
        return replacement(self)
    else:
        raise TypeError(f&#34;Bad type for escape sequence {escape_char}, &#34;
                        &#34;must be &#39;str&#39; or &#39;(Tokenizer) -&gt; str&#39;&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="scrolls.ast" href="index.html">scrolls.ast</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="scrolls.ast.tokenizer.Token" href="#scrolls.ast.tokenizer.Token">Token</a></code></h4>
<ul class="two-column">
<li><code><a title="scrolls.ast.tokenizer.Token.consume_rest" href="#scrolls.ast.tokenizer.Token.consume_rest">consume_rest</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Token.line" href="#scrolls.ast.tokenizer.Token.line">line</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Token.position" href="#scrolls.ast.tokenizer.Token.position">position</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Token.tokenizer" href="#scrolls.ast.tokenizer.Token.tokenizer">tokenizer</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Token.type" href="#scrolls.ast.tokenizer.Token.type">type</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Token.value" href="#scrolls.ast.tokenizer.Token.value">value</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scrolls.ast.tokenizer.Tokenizer" href="#scrolls.ast.tokenizer.Tokenizer">Tokenizer</a></code></h4>
<ul class="">
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.accept_any_of" href="#scrolls.ast.tokenizer.Tokenizer.accept_any_of">accept_any_of</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.accept_comment" href="#scrolls.ast.tokenizer.Tokenizer.accept_comment">accept_comment</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.accept_eof" href="#scrolls.ast.tokenizer.Tokenizer.accept_eof">accept_eof</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.accept_single_char" href="#scrolls.ast.tokenizer.Tokenizer.accept_single_char">accept_single_char</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.accept_string_literal" href="#scrolls.ast.tokenizer.Tokenizer.accept_string_literal">accept_string_literal</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.accept_string_literal_consume_rest" href="#scrolls.ast.tokenizer.Tokenizer.accept_string_literal_consume_rest">accept_string_literal_consume_rest</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.accept_string_literal_normal" href="#scrolls.ast.tokenizer.Tokenizer.accept_string_literal_normal">accept_string_literal_normal</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.accept_string_literal_quoted" href="#scrolls.ast.tokenizer.Tokenizer.accept_string_literal_quoted">accept_string_literal_quoted</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.accept_whitespace" href="#scrolls.ast.tokenizer.Tokenizer.accept_whitespace">accept_whitespace</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.error" href="#scrolls.ast.tokenizer.Tokenizer.error">error</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.forbid_eof" href="#scrolls.ast.tokenizer.Tokenizer.forbid_eof">forbid_eof</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.get_all_tokens" href="#scrolls.ast.tokenizer.Tokenizer.get_all_tokens">get_all_tokens</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.handle_consume_rest" href="#scrolls.ast.tokenizer.Tokenizer.handle_consume_rest">handle_consume_rest</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.handle_consume_rest_consume" href="#scrolls.ast.tokenizer.Tokenizer.handle_consume_rest_consume">handle_consume_rest_consume</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.handle_consume_rest_counting" href="#scrolls.ast.tokenizer.Tokenizer.handle_consume_rest_counting">handle_consume_rest_counting</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.handle_consume_rest_off" href="#scrolls.ast.tokenizer.Tokenizer.handle_consume_rest_off">handle_consume_rest_off</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.next_n_chars" href="#scrolls.ast.tokenizer.Tokenizer.next_n_chars">next_n_chars</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.next_token" href="#scrolls.ast.tokenizer.Tokenizer.next_token">next_token</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.set_comments_enable" href="#scrolls.ast.tokenizer.Tokenizer.set_comments_enable">set_comments_enable</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.set_consume_rest_all" href="#scrolls.ast.tokenizer.Tokenizer.set_consume_rest_all">set_consume_rest_all</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.set_newlines_separate_strings" href="#scrolls.ast.tokenizer.Tokenizer.set_newlines_separate_strings">set_newlines_separate_strings</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.set_quoted_literals_enable" href="#scrolls.ast.tokenizer.Tokenizer.set_quoted_literals_enable">set_quoted_literals_enable</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.set_single_char_token_enable" href="#scrolls.ast.tokenizer.Tokenizer.set_single_char_token_enable">set_single_char_token_enable</a></code></li>
<li><code><a title="scrolls.ast.tokenizer.Tokenizer.try_consume_escape" href="#scrolls.ast.tokenizer.Tokenizer.try_consume_escape">try_consume_escape</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>